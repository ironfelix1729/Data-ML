{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUD-bfxveySk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!find /insert file path/\n",
        "root = 'file path'\n",
        "\n",
        "file_count = sum(len(files) for _, _, files in os.walk(root))\n",
        "print(f\"Total files in deeplob: {file_count}\")"
      ],
      "metadata": {
        "id": "utMOjf-4e42A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "folder = 'folder path'\n",
        "\n",
        "#  Find all logsample and fwdsampler files\n",
        "log_paths = glob.glob(os.path.join(folder, '*log_sample*'))\n",
        "fwd_paths = glob.glob(os.path.join(folder, '*fwdsampler*'))\n",
        "\n",
        "#  Helper to get date out of filename\n",
        "def extract_date(filepath):\n",
        "    base = os.path.basename(filepath)\n",
        "    name, _ = os.path.splitext(base)\n",
        "\n",
        "    clean = re.sub(r'(?i)^copy of\\s*', '', name)\n",
        "    m = re.match(r'(\\d{8})', clean)\n",
        "\n",
        "    return pd.to_datetime(m.group(1), format='%Y%m%d').date()\n",
        "\n",
        "# Build a mapping date  {'log': path, 'fwd': path}\n",
        "by_date = {}\n",
        "for p in log_paths:\n",
        "    d = extract_date(p)\n",
        "    by_date.setdefault(d, {})['log'] = p\n",
        "for p in fwd_paths:\n",
        "    d = extract_date(p)\n",
        "    by_date.setdefault(d, {})['fwd'] = p\n",
        "print(by_date)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CHxwP5OTfCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import pyarrow\n",
        "\n",
        "#  the source directory containing CSV files\n",
        "csv_dir = 'insert file path'\n",
        "\n",
        "#the new target directory for Parquet files\n",
        "parquet_new_dir = 'insert file path'\n",
        "\n",
        "# Create the new target directory if it doesn't exist\n",
        "os.makedirs(parquet_new_dir, exist_ok=True)\n",
        "print(f\"Ensured target directory exists: {parquet_new_dir}\")\n",
        "\n",
        "# Check if the source directory exists before listing\n",
        "if not os.path.exists(csv_dir):\n",
        "    print(f\"Error: Source directory not found: {csv_dir}\")\n",
        "else:\n",
        "    print(f\"Processing files from: {csv_dir}\")\n",
        "    for csv_file in os.listdir(csv_dir):\n",
        "\n",
        "\n",
        "        csv_path = os.path.join(csv_dir, csv_file)\n",
        "\n",
        "        # Define the output filename in the new directory\n",
        "        parquet_fname = csv_file + '.parquet'\n",
        "        parquet_path = os.path.join(parquet_new_dir, parquet_fname)\n",
        "\n",
        "\n",
        "\n",
        "        # Read the CSV file into a pandas DataFrame\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "            print(f\"Read {csv_file}: {df.shape[0]} rows\")\n",
        "\n",
        "            # Save the DataFrame to Parquet format in the new directory\n",
        "            df.to_parquet(parquet_path,\n",
        "                          engine='pyarrow',\n",
        "                          compression='snappy')\n",
        "\n",
        "            print(f\"Saved {parquet_fname} to {parquet_new_dir}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {csv_file}: {e}\")\n",
        "\n",
        "# List files in the new directory\n",
        "print(\"\\nContents of the new directory:\")\n",
        "if os.path.exists(parquet_new_dir):\n",
        "    for fname in os.listdir(parquet_new_dir):\n",
        "        print(f\"  {fname}\")\n",
        "else:\n",
        "    print(f\"Target directory {parquet_new_dir} was not created or is empty.\")"
      ],
      "metadata": {
        "id": "oJ6LCLw3iRlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing windows of length less than 100 and truncating the ones with length>100 and keeping the last 100 enteries\n",
        "import os\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "import numpy as np\n",
        "\n",
        "processed_parquet_dir = '/folder path'\n",
        "\n",
        "target_column = 'first_tp'\n",
        "\n",
        "# Check if the directory exists before listing\n",
        "if not os.path.exists(processed_parquet_dir):\n",
        "    print(f\"Error: Directory not found: {processed_parquet_dir}\")\n",
        "else:\n",
        "    print(f\"Processing files in: {processed_parquet_dir}\")\n",
        "    for pq_file in os.listdir(processed_parquet_dir):\n",
        "        if not pq_file.endswith('.log_sample.parquet'):\n",
        "            continue\n",
        "\n",
        "        # Construct the full path to the Parquet file\n",
        "        file_path = os.path.join(processed_parquet_dir, pq_file)\n",
        "\n",
        "        if not os.path.isfile(file_path):\n",
        "            print(f\"Skipping {pq_file}: Not a file.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nProcessing file: {pq_file}\")\n",
        "\n",
        "        # Read the Parquet file into a pandas DataFrame\n",
        "        try:\n",
        "            df = pd.read_parquet(file_path)\n",
        "            initial_rows = df.shape[0]\n",
        "            print(f\"Initial rows: {initial_rows}\")\n",
        "\n",
        "            if target_column not in df.columns:\n",
        "                print(f\"  Warning: Target column '{target_column}' not found in {pq_file}. Skipping processing.\")\n",
        "                continue\n",
        "\n",
        "            # Get chunk sizes to see what we're working with\n",
        "            chunk_sizes = df.groupby(target_column).size()\n",
        "\n",
        "            if chunk_sizes.empty:\n",
        "                print(f\"  Warning: No data found in column '{target_column}' for {pq_file}. No rows to filter.\")\n",
        "                df_filtered = df.copy()\n",
        "            else:\n",
        "                # Print some stats about what we found\n",
        "                total_chunks = len(chunk_sizes)\n",
        "                chunks_under_100 = (chunk_sizes < 100).sum()\n",
        "                chunks_over_100 = (chunk_sizes > 100).sum()\n",
        "                chunks_exactly_100 = (chunk_sizes == 100).sum()\n",
        "\n",
        "                print(f\"  Found {total_chunks} total chunks:\")\n",
        "                print(f\"    {chunks_under_100} chunks < 100 rows (will drop)\")\n",
        "                print(f\"    {chunks_exactly_100} chunks = 100 rows (will keep)\")\n",
        "                print(f\"    {chunks_over_100} chunks > 100 rows (will truncate)\")\n",
        "\n",
        "                # Keep only chunks that have exactly 100 or more rows, then truncate to last 100\n",
        "                # This is more efficient than the old loop approach\n",
        "                valid_chunks = chunk_sizes[chunk_sizes >= 100].index\n",
        "\n",
        "                if len(valid_chunks) == 0:\n",
        "                    print(\"  No valid chunks found, creating empty dataframe\")\n",
        "                    df_filtered = pd.DataFrame(columns=df.columns)\n",
        "                else:\n",
        "                    # Vectorized approach - much faster than loops\n",
        "                    df_filtered = (df[df[target_column].isin(valid_chunks)]\n",
        "                                   .groupby(target_column, group_keys=False)\n",
        "                                   .tail(100)  # Keep last 100 rows per group\n",
        "                                   .reset_index(drop=True))\n",
        "\n",
        "            filtered_rows = len(df_filtered)\n",
        "            dropped_rows = initial_rows - filtered_rows\n",
        "\n",
        "            if dropped_rows > 0:\n",
        "                print(f\"Filtered: Dropped {dropped_rows} rows ({initial_rows} -> {filtered_rows}).\")\n",
        "            else:\n",
        "                print(\"Filtered: No rows dropped.\")\n",
        "\n",
        "            # Always save if we had data to begin with\n",
        "            if initial_rows > 0:\n",
        "                print(f\"Overwriting {pq_file} with filtered data...\")\n",
        "                df_filtered.to_parquet(file_path,\n",
        "                                      engine='pyarrow',\n",
        "                                      compression='snappy')\n",
        "                print(f\"Successfully saved filtered {pq_file}\")\n",
        "            else:\n",
        "                print(\"File was empty initially, no file written.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pq_file}: {e}\")\n",
        "\n",
        "print(\"\\nProcessing complete.\")"
      ],
      "metadata": {
        "id": "Xt_TFY0ViyJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aligns pairs of \"log_sample\" and \"fwdsampler\" files based on matching timestamps, ensuring that for each 100-row chunk in a logbook file, there's a corresponding timestamp in the fwdsampler file. If a logbook chunk doesn't have a matching timestamp in the fwdsampler file (specifically for its 100th row), that entire logbook chunk is removed, and the corresponding rows in the fwdsampler file are also removed.\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "import numpy as np\n",
        "\n",
        "# Define the directory containing the Parquet files\n",
        "parquet_dir = 'file path'\n",
        "\n",
        "chunk_id_column = 'first_tp' #  'first_tp' identifies the chunks\n",
        "\n",
        "# The column that counts the position within each chunk (should reach 100 for the last row of a full chunk)\n",
        "counter_column = 'counter'\n",
        "\n",
        "# The timestamp column name\n",
        "timestamp_column = 'time'\n",
        "\n",
        "#  files are named YYYYMMDD.log_sample.parquet and YYYYMMDD.fwdsampler.parquet\n",
        "\n",
        "# 1) Group files by date prefix\n",
        "files_by_date = {}\n",
        "if os.path.exists(parquet_dir):\n",
        "    for fname in os.listdir(parquet_dir):\n",
        "        if not fname.endswith('.parquet'):\n",
        "            continue\n",
        "        parts = fname.split('.')\n",
        "        if len(parts) < 3:\n",
        "             continue\n",
        "        date_str = parts[0]\n",
        "        file_type = parts[1]\n",
        "\n",
        "        if len(date_str) == 8 and file_type in ['log_sample', 'fwdsampler']:\n",
        "             date_path = files_by_date.setdefault(date_str, {})\n",
        "             date_path[file_type] = os.path.join(parquet_dir, fname)\n",
        "else:\n",
        "     print(f\"Error: Directory not found: {parquet_dir}\")\n",
        "     files_by_date = {}\n",
        "\n",
        "print(f\"Found {len(files_by_date)} dates with files in {parquet_dir}\")\n",
        "\n",
        "# print(f\"\\nAligning Logbook chunks ({chunk_id_column}/{counter_column}) with Fwdsampler timestamps...\")\n",
        "\n",
        "# 2) Iterate through dates and process pairs\n",
        "for date_str, paths in sorted(files_by_date.items()):\n",
        "    log_path = paths.get('log_sample')\n",
        "    fwd_path = paths.get('fwdsampler')\n",
        "\n",
        "    if not log_path or not fwd_path:\n",
        "        print(f\"\\nSkipping date {date_str}: Missing log_sample or fwdsampler file.\")\n",
        "        continue\n",
        "\n",
        "    # Flag to track if either file is modified for this date\n",
        "    modified_this_date = False\n",
        "    dropped_log_rows = 0 # Initialize dropped counts\n",
        "    dropped_fwd_rows = 0\n",
        "\n",
        "    try:\n",
        "        df_log = pd.read_parquet(log_path)\n",
        "        initial_log_rows = len(df_log)\n",
        "\n",
        "        df_fwd = pd.read_parquet(fwd_path)\n",
        "        initial_fwd_rows = len(df_fwd)\n",
        "\n",
        "        # Check required columns in logbook\n",
        "        missing_log_cols = [col for col in [chunk_id_column, counter_column, timestamp_column] if col not in df_log.columns]\n",
        "        if missing_log_cols:\n",
        "            print(f\"  Warning: Logbook missing required columns {missing_log_cols} in {os.path.basename(log_path)} for date {date_str}. Skipping alignment for this date.\")\n",
        "            continue\n",
        "\n",
        "        # Check required columns in fwdsampler\n",
        "        if timestamp_column not in df_fwd.columns:\n",
        "             print(f\"  Warning: Fwdsampler missing '{timestamp_column}' column in {os.path.basename(fwd_path)} for date {date_str}. Skipping alignment for this date.\")\n",
        "             continue\n",
        "\n",
        "\n",
        "        #  Determine Logbook Timestamps to Match\n",
        "\n",
        "        # Filter logbook to find the rows where the counter column is 100\n",
        "        # Also check that these rows belong to chunks that have exactly 100 rows in total\n",
        "        chunk_counts = df_log[chunk_id_column].value_counts()\n",
        "        valid_chunks_by_count = chunk_counts[chunk_counts == 100].index.tolist()\n",
        "\n",
        "        # Filter logbook rows: counter == 100 AND chunk ID is in the list of 100-row chunks\n",
        "        log_100th_rows = df_log[\n",
        "            (df_log[counter_column] == 100) &\n",
        "            (df_log[chunk_id_column].isin(valid_chunks_by_count))\n",
        "        ].copy()\n",
        "\n",
        "        if log_100th_rows.empty:\n",
        "             # No 100th rows found for 100-row chunks → no timestamps to match → no chunks to keep\n",
        "             chunk_ids_with_matching_fwd_ts = set()\n",
        "             fwd_timestamps_to_keep = set() # No timestamps to match\n",
        "        else:\n",
        "            # Extract the timestamps from these 100th rows\n",
        "            # Try the new merge approach - should be faster than the old set operations\n",
        "            timestamp_matches = pd.merge(\n",
        "                log_100th_rows[[chunk_id_column, timestamp_column]].drop_duplicates(),\n",
        "                df_fwd[[timestamp_column]].drop_duplicates(),\n",
        "                on=timestamp_column,\n",
        "                how='inner'\n",
        "            )\n",
        "\n",
        "            chunk_ids_with_matching_fwd_ts = set(timestamp_matches[chunk_id_column])\n",
        "            fwd_timestamps_to_keep = set(timestamp_matches[timestamp_column])\n",
        "\n",
        "            # debug: let's see how many matches we got\n",
        "            # print(f\"  Debug: Found {len(timestamp_matches)} timestamp matches\")\n",
        "\n",
        "        # Filter the original logbook DataFrame based on matching chunk IDs\n",
        "        df_log_aligned = df_log[\n",
        "            df_log[chunk_id_column].isin(chunk_ids_with_matching_fwd_ts)\n",
        "        ].copy()\n",
        "        aligned_log_rows = len(df_log_aligned)\n",
        "        dropped_log_rows = initial_log_rows - aligned_log_rows\n",
        "\n",
        "        # Filter the fwdsampler DataFrame based on the matched timestamps\n",
        "        df_fwd_aligned = df_fwd[\n",
        "            df_fwd[timestamp_column].isin(fwd_timestamps_to_keep)\n",
        "        ].copy()\n",
        "        aligned_fwd_rows = len(df_fwd_aligned)\n",
        "        dropped_fwd_rows = initial_fwd_rows - aligned_fwd_rows\n",
        "\n",
        "        # Determine if any modifications were made\n",
        "        modified_this_date = (dropped_log_rows > 0) or (dropped_fwd_rows > 0)\n",
        "\n",
        "        # Print Summary and Overwrite if Modified\n",
        "        if modified_this_date:\n",
        "             print(f\"\\nProcessing date {date_str}: Aligning {os.path.basename(log_path)} and {os.path.basename(fwd_path)}.\")\n",
        "             # Print relevant info if modifications happened\n",
        "             print(f\"  Found {len(chunk_counts)} unique {chunk_id_column} values initially.\")\n",
        "             print(f\"  Found {len(valid_chunks_by_count)} chunks with exactly 100 rows in logbook initially.\")\n",
        "             print(f\"  Identified {len(chunk_ids_with_matching_fwd_ts)} chunks whose 100th row timestamp matched a fwdsampler timestamp.\")\n",
        "\n",
        "\n",
        "             # Logbook modification details\n",
        "             if dropped_log_rows > 0:\n",
        "                 print(f\"  Logbook initial rows: {initial_log_rows}\")\n",
        "                 print(f\"  Aligned: Dropped {dropped_log_rows} rows ({initial_log_rows} -> {aligned_log_rows}) from {os.path.basename(log_path)}.\")\n",
        "                 print(f\"  Overwriting {os.path.basename(log_path)} with aligned data...\")\n",
        "                 df_log_aligned.to_parquet(log_path,\n",
        "                                          engine='pyarrow',\n",
        "                                          compression='snappy')\n",
        "                 print(f\"  Successfully saved aligned {os.path.basename(log_path)}\")\n",
        "             else:\n",
        "                  print(f\"  Logbook ({os.path.basename(log_path)}) required no row dropping based on chunk timestamp alignment.\") # Indicate no logbook change\n",
        "\n",
        "\n",
        "             # Fwdsampler modification details\n",
        "             if dropped_fwd_rows > 0:\n",
        "                 print(f\"  Fwdsampler initial rows: {initial_fwd_rows}\")\n",
        "                 print(f\"  Aligned: Dropped {dropped_fwd_rows} rows ({initial_fwd_rows} -> {aligned_fwd_rows}) from {os.path.basename(fwd_path)} to match kept logbook chunk 100th timestamps.\")\n",
        "                 print(f\"  Overwriting {os.path.basename(fwd_path)} with aligned data...\")\n",
        "                 df_fwd_aligned.to_parquet(fwd_path,\n",
        "                                          engine='pyarrow',\n",
        "                                          compression='snappy')\n",
        "                 print(f\"  Successfully saved aligned {os.path.basename(fwd_path)}\")\n",
        "             else:\n",
        "                  print(f\"  Fwdsampler ({os.path.basename(fwd_path)}) required no row dropping based on logbook chunk timestamp alignment.\") # Indicate no fwdsampler change\n",
        "\n",
        "        else:\n",
        "             # Print a single message if neither file was modified for this date\n",
        "             print(f\"\\nDate {date_str}: No modifications needed for {os.path.basename(log_path)} and {os.path.basename(fwd_path)} based on chunk timestamp alignment.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Error processing date {date_str}: {e}\")\n",
        "\n",
        "print(\"\\nChunk timestamp alignment process complete.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ChjCXlpPnw3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#file sizes comparison\n",
        "import os\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# Define the directory containing the cleaned and aligned Parquet files\n",
        "parquet_dir = 'file path'\n",
        "\n",
        "# 1) Group files by date prefix\n",
        "files_by_date = {}\n",
        "if os.path.exists(parquet_dir):\n",
        "    for fname in os.listdir(parquet_dir):\n",
        "        if not fname.endswith('.parquet'):\n",
        "            continue\n",
        "        parts = fname.split('.')\n",
        "\n",
        "\n",
        "        date_str = parts[0]\n",
        "        # The type is the part before .parquet, e.g., log_sample or fwdsampler\n",
        "        file_type = parts[1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Found {len(files_by_date)} dates with files in {parquet_dir}\")\n",
        "\n",
        "\n",
        "print(\"\\nChecking Logbook vs Fwdsampler Row Counts...\")\n",
        "\n",
        "# 2) Iterate through dates and print row counts side-by-side\n",
        "# Sort by date string for consistent output\n",
        "for date_str, paths in sorted(files_by_date.items()):\n",
        "    log_path = paths.get('log_sample')\n",
        "    fwd_path = paths.get('fwdsampler')\n",
        "\n",
        "    log_rows = \"N/A\" # Initialize counts as N/A\n",
        "    fwd_rows = \"N/A\"\n",
        "\n",
        "    if log_path and os.path.exists(log_path):\n",
        "        try:\n",
        "            # Use pyarrow.parquet to efficiently get row count from metadata\n",
        "            log_pf = pq.ParquetFile(log_path)\n",
        "            log_rows = log_pf.metadata.num_rows\n",
        "        except Exception as e:\n",
        "            log_rows = f\"Error ({e})\"\n",
        "\n",
        "    if fwd_path and os.path.exists(fwd_path):\n",
        "         try:\n",
        "            # Use pyarrow.parquet to efficiently get row count from metadata\n",
        "            fwd_pf = pq.ParquetFile(fwd_path)\n",
        "            fwd_rows = fwd_pf.metadata.num_rows\n",
        "         except Exception as e:\n",
        "            fwd_rows = f\"Error ({e})\"\n",
        "\n",
        "    # Print the results for the date\n",
        "    print(f\"  Date {date_str}: Log rows: {log_rows}, Fwd rows: {fwd_rows}\")\n",
        "\n",
        "print(\"\\nRow count check complete.\")"
      ],
      "metadata": {
        "id": "ycsaehCxnbwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "-----------------------------------------------------------\n",
        "ONE-PASS loader that builds **aligned** book-and-tick tensors\n",
        "and the target vector straight from the cleaned  *.parquet files.\n",
        "\n",
        " Normalises prices by (price – fwd.mid) / fwd.mid\n",
        "• Normalises sizes and trade qty by (value / totalDepth@t=99)\n",
        "• Streams Parquet in ≤200 k-row batches → fits 15 M rows on 32 GB RAM\n",
        "• Drops any window whose row-99 ‘time’ key is missing in the\n",
        "  forward-sampler file (rare data glitches)\n",
        "-----------------------------------------------------------\n",
        "Usage\n",
        "------\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pyarrow.dataset as ds\n",
        "import torch, gc, warnings\n",
        "\n",
        "import numpy as np, torch, pyarrow.dataset as ds, gc\n",
        "from pathlib import Path\n",
        "\n",
        "PRICE_COLS = [f\"{s}_price{i}\" for s in (\"ask\",\"bid\") for i in range(10)]\n",
        "SIZE_COLS  = [f\"{s}_size{i}\"  for s in (\"ask\",\"bid\") for i in range(10)]\n",
        "LOB_COLS   = PRICE_COLS + SIZE_COLS\n",
        "TICK_RAW   = [\"tick_type\", \"side\", \"price\", \"quantity\", \"old_price\"]\n",
        "ALL_COLS   = [\"time\", \"first_tp\"] + LOB_COLS + TICK_RAW\n",
        "TT = {\"NEW\":0,\"MODIFY\":1,\"TRADE\":2,\"CANCEL\":3}\n",
        "SD = {\"Buy\":0,\"Sell\":1}\n",
        "\n",
        "def day_fast(date:str, raw_dir:Path, batch_rows=200_000):\n",
        "    log_p = raw_dir/f\"{date}.log_sample.parquet\"\n",
        "    fwd_p = raw_dir/f\"{date}.fwdsampler.parquet\"\n",
        "\n",
        "    fwd = ds.dataset(fwd_p, format=\"parquet\").to_table().to_pandas()[[\"time\",\"mid\",\"mid_10.000s\"]]\n",
        "    fwd[\"target\"] = (fwd[\"mid_10.000s\"]-fwd[\"mid\"])/fwd[\"mid\"]\n",
        "    fwd_lookup = fwd.set_index(\"time\")\n",
        "    valid_times = fwd_lookup.index.to_numpy()\n",
        "\n",
        "    scanner = ds.dataset(log_p, format=\"parquet\").scanner(columns=ALL_COLS,\n",
        "                                                          batch_size=batch_rows)\n",
        "\n",
        "    rows_out, tgt_out = [], []\n",
        "\n",
        "    for batch in scanner.to_batches():\n",
        "        df = batch.to_pandas()\n",
        "\n",
        "        # ------- drop rows whose *last* time not in fwd (cheap pre-filter)\n",
        "        #mask_keep = np.isin(df[\"time\"].values, valid_times, assume_unique=False)\n",
        "        #df = df.loc[mask_keep]\n",
        "\n",
        "        #if df.empty: continue\n",
        "\n",
        "        # ------- block index 0-99 per time key\n",
        "        df[\"idx\"] = df.groupby(\"first_tp\").cumcount()\n",
        "        complete  = df[\"idx\"]==99\n",
        "        ref_rows  = df.loc[complete, [\"first_tp\", \"time\"] + SIZE_COLS]\n",
        "        # merge ref_rows with fwd mid / target\n",
        "        ref = ref_rows.merge(fwd_lookup, left_on=\"time\", right_index=True, how=\"inner\")\n",
        "        if ref.empty:\n",
        "          print(f\"DROPPED BATCH: {len(ref_rows)} complete windows\")\n",
        "          print(f\"Sample ref times: {ref_rows['time'].head().values}\")\n",
        "          print(f\"Sample fwd times: {fwd_lookup.index[:5].values}\")   # no full blocks in this chunk\n",
        "          continue\n",
        "\n",
        "        # -------- compute per-row broadcast constants -----------------\n",
        "        mid = ref[\"mid\"].to_numpy(np.float32)              # (W,)\n",
        "        vol = ref[SIZE_COLS].to_numpy(np.float32).reshape(-1,20).sum(1)\n",
        "        vol[vol<=0] = 1.0\n",
        "\n",
        "        # index array of 100-row slices we keep\n",
        "        keep_ftps  = ref[\"first_tp\"].values\n",
        "        mask_win   = np.isin(df[\"first_tp\"].values, keep_ftps, assume_unique=False)\n",
        "        block = df.loc[mask_win].sort_values([\"first_tp\",\"idx\"])\n",
        "\n",
        "        # reshape to (W,100,…) using np.column_stack → much faster than loops\n",
        "        W = len(keep_ftps)\n",
        "        book = block[LOB_COLS].to_numpy(np.float32).reshape(W,100,40)\n",
        "        tick = np.zeros((W,100,9), dtype=np.float32)\n",
        "\n",
        "        # ---------- vectorised tick encoding ---------------------------\n",
        "        # ---------- vectorised tick encoding (safe) ----------------------------\n",
        "        tt = block[\"tick_type\"].map(TT).fillna(-1).astype(np.int8).values\n",
        "        sd = block[\"side\"     ].map(SD).fillna(-1).astype(np.int8).values\n",
        "        tt = tt.reshape(W, 100)\n",
        "        sd = sd.reshape(W, 100)\n",
        "\n",
        "        tick[:] = 0.0                             # ensure zero-fill\n",
        "        # ---------- one-hot NEW / MODIFY / TRADE / CANCEL -----------------\n",
        "        mask_tt = tt >= 0\n",
        "        if mask_tt.any():\n",
        "           w_idx, t_idx = np.nonzero(mask_tt)          # same length L\n",
        "           tick[w_idx, t_idx, tt[w_idx, t_idx]] = 1.0\n",
        "\n",
        "# ---------- one-hot Buy / Sell ------------------------------------\n",
        "        mask_sd = sd >= 0\n",
        "        if mask_sd.any():\n",
        "           w_idx, t_idx = np.nonzero(mask_sd)\n",
        "           tick[w_idx, t_idx, 4 + sd[w_idx, t_idx]] = 1.0\n",
        "\n",
        "\n",
        "\n",
        "        col = lambda c: block[c].to_numpy(np.float32).reshape(W,100)\n",
        "        tick[:,:,6] = col(\"price\")\n",
        "        tick[:,:,7] = col(\"quantity\")\n",
        "        tick[:,:,8] = col(\"old_price\")\n",
        "\n",
        "        # ---------- reference normalisation ----------------------------\n",
        "        mid_bc = mid[:,None,None]             # (W,1,1)\n",
        "        vol_bc = vol[:,None,None]\n",
        "\n",
        "        book[:,:,:20] = (book[:,:,:20] - mid_bc) / mid_bc\n",
        "        book[:,:, 20:] = book[:,:,20:] / vol_bc\n",
        "\n",
        "        mask_old = tick[:,:,8]!=0\n",
        "        mid_plane = np.repeat(mid[:, None], 100, axis=1)   # (W,100)  not (W,1)\n",
        "              # (W,1)  → will broadcast to (W,100)\n",
        "\n",
        "        tick[:, :, 6] = (tick[:, :, 6] - mid_plane) / mid_plane     # broadcasts OK\n",
        "        mask_old = tick[:, :, 8] != 0\n",
        "        tick[:, :, 8][mask_old] = (tick[:, :, 8][mask_old] - mid_plane[mask_old]) / mid_plane[mask_old]\n",
        "        tick[:, :, 7] /= vol[:, None]\n",
        "\n",
        "\n",
        "        rows_out.append(np.concatenate([book,tick],axis=2))  # (W,100,49)\n",
        "        tgt_out.append(ref[\"target\"].to_numpy(np.float32))\n",
        "\n",
        "        del df, block, book, tick, ref, mid, vol\n",
        "        gc.collect()\n",
        "\n",
        "    X = torch.from_numpy(np.concatenate(rows_out,0))\n",
        "    y = torch.from_numpy(np.concatenate(tgt_out,0))\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────── multi-day convenience wrapper ──\n",
        "def dates_to_dataset(dates: list[str], raw_dir: str | Path,\n",
        "                     gpu: bool = False\n",
        "                     ) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Load*-normalise*-stack several days in order.\n",
        "    `gpu=True` → final tensors moved to CUDA with non-blocking copy.\n",
        "    \"\"\"\n",
        "    raw_dir = Path(raw_dir)\n",
        "    xs, ys  = [], []\n",
        "    for d in dates:\n",
        "        print(f\"⋯ {d}\", end=\"\", flush=True)\n",
        "        Xi, yi = day_fast(d, RAW_DIR)\n",
        "\n",
        "        xs.append(Xi); ys.append(yi)\n",
        "        print(f\"  ✓ {Xi.size(0):,} windows\")\n",
        "    X = torch.cat(xs, 0)\n",
        "    y = torch.cat(ys, 0)\n",
        "    if gpu and torch.cuda.is_available():\n",
        "        X = X.cuda(non_blocking=True)\n",
        "        y = y.cuda(non_blocking=True)\n",
        "    return X, y\n",
        "\n",
        "# ───────────────────────────────────────────────────────────── quick demo ──\n",
        "if __name__ == \"__main__\":\n",
        "    RAW_DIR      = Path(\"/content/drive/MyDrive/new_data_parquet\")\n",
        "    TRAIN_DATES  = sorted({p.name[:8] for p in RAW_DIR.glob(\"*.log_sample.parquet\")})[:-6]\n",
        "    TEST_DATES   = sorted({p.name[:8] for p in RAW_DIR.glob(\"*.log_sample.parquet\")})[-6:]\n",
        "\n",
        "    print(\"Loading train …\")\n",
        "    X_train, y_train = dates_to_dataset(TRAIN_DATES, RAW_DIR, gpu=False)\n",
        "    print(\"Loading test  …\")\n",
        "    X_test , y_test  = dates_to_dataset(TEST_DATES , RAW_DIR, gpu=False)\n",
        "\n",
        "    print(\"\\nFinal shapes\")\n",
        "    print(\"X_train\", X_train.shape, \"y_train\", y_train.shape)\n",
        "    print(\"X_test \", X_test.shape , \"y_test \", y_test.shape)\n"
      ],
      "metadata": {
        "id": "FwtTIZkJw0Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "\n",
        "def add_cross_and_normalise(X_train, X_test):\n",
        "    \"\"\"\n",
        "    X_train : (N_train, 100, 49)  – already price/size-normalised\n",
        "    X_test  : (N_test, 100, 49)\n",
        "    returns\n",
        "        X_train_z : (N_train, 100, 69)\n",
        "        X_test_z  : (N_test, 100, 69)\n",
        "    \"\"\"\n",
        "\n",
        "    # ----- 1. build cross-products ------------------------------------\n",
        "    def cross_block(X):\n",
        "        # slices\n",
        "        ask_p  = X[...,  :10]     # (N,100,10)\n",
        "        bid_p  = X[..., 10:20]\n",
        "        ask_sz = X[..., 20:30]\n",
        "        bid_sz = X[..., 30:40]\n",
        "\n",
        "        cross_1 = ask_p * bid_sz  # askPriceᵢ × bidSizeᵢ\n",
        "        cross_2 = bid_p * ask_sz  # bidPriceᵢ × askSizeᵢ\n",
        "        return torch.cat([X, cross_1, cross_2], dim=-1)   # (N,100,69)\n",
        "\n",
        "    X_train_aug = cross_block(X_train)\n",
        "    X_test_aug  = cross_block(X_test)\n",
        "\n",
        "    # ----- 2. Z-score statistics from TRAIN only ----------------------\n",
        "    # flatten (N,100) for mean/var per channel\n",
        "    mean = X_train_aug.mean(dim=(0,1), keepdim=True)      # (1,1,69)\n",
        "    std  = X_train_aug.std (dim=(0,1), keepdim=True).clamp_min(1e-6)\n",
        "\n",
        "    # ----- 3. apply ---------------------------------------------------\n",
        "    X_train_z = (X_train_aug - mean) / std\n",
        "    X_test_z  = (X_test_aug  - mean) / std\n",
        "\n",
        "\n",
        "    del X_train_aug, X_test_aug\n",
        "    gc.collect()\n",
        "\n",
        "    return X_train_z, X_test_z\n"
      ],
      "metadata": {
        "id": "dZT9DRLCxYEu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}